{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Extractor de Características de Audio\n",
        "Con este algoritmo procesaremos las bases de datos de audios (formato wav.) y pasaremos a la conversión de estos a expectrograma Mel.\n"
      ],
      "metadata": {
        "id": "VeDJLrsyjgnn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Librerías que se utilizarán"
      ],
      "metadata": {
        "id": "YXr0giWQk4B3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "#librería para funciones matemáticas básicas\n",
        "import numpy as np\n",
        "#librería para trabajar con arreglos numéricos y realizar operaciones matemáticas en ellos\n",
        "import torch\n",
        "#librería de aprendizaje profundo de PyTorch para trabajar con redes neuronales y tensores\n",
        "import torch.nn as nn\n",
        "#módulo de PyTorch para definir arquitecturas de redes neuronales\n",
        "import torch.nn.functional as F\n",
        "#módulo de PyTorch con funciones de activación y otras operaciones comunes en redes neuronales\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "!pip install seaborn\n",
        "from torch.utils.data import DataLoader, random_split, SubsetRandomSampler\n",
        "import os\n",
        "import glob\n",
        "from IPython.display import Audio\n",
        "import librosa.display\n",
        "import pandas as pd\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "from torch.utils.data import TensorDataset\n",
        "from sklearn.metrics import confusion_matrix #para la función kfold\n",
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "import importlib\n",
        "import matplotlib_inline\n",
        "import pickle\n",
        "\n",
        "#Para cargar la base de datos desde drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "ZejW-ckDkBm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Carga de datasets\n",
        "En este código se procederá a:<br>\n",
        "<font color=pink><br>-Cargar los datos de audio.<br>\n",
        "<br>-Obtener las muestras y la frecuencia de muestreo de estas (para el cálculo posterior de los espectrogramas-Mel).<br>\n",
        "<br>-Mediante la librería Librosa, extraer los espectrogramas Mel de las señales de audio.<br>\n",
        "<br>-Etiquetar los audios con categorías numéricas asignándoles labels a cada emoción.<br>\n",
        "<br>-Aumentar los datos mediante la técnica de adición de ruido para balancear los datos de entrada del modelo.<br>\n",
        "<br>-Guardar los datos procesados.<br>\n",
        "<br>-Mostrar en un histograma la cantidad de datos por etiquetado."
      ],
      "metadata": {
        "id": "AT-4M5w1kQTf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Cargamos en tpath los archivos .wav desde el drive.\n",
        "tpath = (\"/content/drive/MyDrive/Datasets/TESS Toronto emotional speech set data\")\n",
        "data = []\n",
        "audio = []\n",
        "labels = []\n",
        "nombres=[]\n",
        "\n",
        "name = [x[0] for x in os.walk(tpath)]\n",
        "name.pop(0)\n",
        "#Inicializamos la variable donde se guardarán los espectrogramas.\n",
        "spectrograms = np.zeros((1, 75, 128))\n",
        "\n",
        "\n",
        "#Recorremos el archivo donde están guardados los audios.\n",
        "for i,names in enumerate(name):\n",
        "  print(names)\n",
        "  file_path = names\n",
        "  files = os.listdir(file_path)\n",
        "\n",
        "  for j, filename in enumerate(glob.glob(os.path.join(file_path, '*.wav'))):\n",
        "    x = filename.split(\"/\")\n",
        "    y = x[7].split(\"_\")\n",
        "    nombre = y[1] #Obtenemos los nombres de los audios.\n",
        "    print(str(nombre))\n",
        "    nombres.append(nombre)\n",
        "    z=y[-1].split(\".\")\n",
        "    id= z[0] #Se obtiene el identificador de la emoción.\n",
        "    print(\"id\",id)\n",
        "\n",
        "    #Obtenemos las muestras y frecuencias de muestreo de los audios.\n",
        "    sample, sr = librosa.load(filename)\n",
        "    audio.append(sample)\n",
        "    duration = len(sample)/sr\n",
        "\n",
        "    #Se extraen los Mel-espectrogramas de las muestras de audio con el uso de la librería Librosa.\n",
        "    spec = librosa.feature.melspectrogram(\n",
        "        y= sample,\n",
        "        sr=sr,\n",
        "        n_fft= 1024,\n",
        "        hop_length= 512,\n",
        "        n_mels= 128,\n",
        "        fmax= 8000,\n",
        "        win_length= 400\n",
        "    )\n",
        "\n",
        "\n",
        "    #Hacemos la transpuesta a los espectrogramas y los pasamos a decibelios para proceder con la división en segmentos de estos espectrogramas (longitud=75).\n",
        "    spec = librosa.power_to_db(spec).transpose()\n",
        "    print(spec.shape)\n",
        "    div = (spec.shape[0] // 75)\n",
        "    print(spec.shape[0])\n",
        "    size_new = int(div*75)\n",
        "    spec = spec[:size_new]\n",
        "    spec = spec.reshape((div, 75, 128))\n",
        "\n",
        "\n",
        "\n",
        "    #Etiquetamos cada identificador de cada segmento de espectrograma, el cuál dependiendo del id corresponde a una u otra emoción, un valor entero.\n",
        "    for k in range(div):\n",
        "      if (id == \"Fear\") or (id == \"fear\"):\n",
        "        lab = int(6)\n",
        "      elif (id == \"Pleasant\") or (id == \"pleasant\") or (id == \"ps\"):\n",
        "        lab = int(5)\n",
        "      elif (id == \"Sad\") or (id == \"sad\"):\n",
        "        lab = int(4)\n",
        "      elif id == \"angry\":\n",
        "        lab = int(3)\n",
        "      elif id == \"disgust\":\n",
        "        lab = int(2)\n",
        "      elif id == \"happy\":\n",
        "        lab = int(1)\n",
        "      else:\n",
        "        lab = int(0)\n",
        "      labels.append(lab)\n",
        "\n",
        "    spectrograms = np.concatenate((spectrograms, spec))\n",
        "    print(\"Label: \"+str(lab)+\"\\tSpectrogram shape: \"+str(spec.shape) )\n",
        "    print()\n",
        "\n",
        "\n",
        "    #Pasamos al aumento de datos mediante la técnica de adición de ruido.\n",
        "    if (lab == 3) or (lab == 6):\n",
        "      y_noise = sample\n",
        "      rms = math.sqrt(np.mean(y_noise**2))\n",
        "      noise = np.random.normal(0, rms, y_noise.shape[0])\n",
        "      y_noise = y_noise + noise\n",
        "      spec_t = librosa.feature.melspectrogram(\n",
        "        y= sample,\n",
        "        sr=sr,\n",
        "        n_fft= 1024,\n",
        "        hop_length= 512,\n",
        "        n_mels= 128,\n",
        "        fmax= 8000,\n",
        "        win_length= 400\n",
        "      )\n",
        "      spec_t = librosa.power_to_db(spec_t).transpose()\n",
        "      div = (spec_t.shape[0] // 75)\n",
        "      size_new = int(div*75)\n",
        "      spec_t = spec_t[:size_new]\n",
        "      spec_t = spec_t.reshape((div, 75, 128))\n",
        "      for k in range(div):\n",
        "        labels.append(lab)\n",
        "      spectrograms = np.concatenate((spectrograms, spec_t))\n",
        "\n",
        "\n",
        "    if (lab == 6):\n",
        "      y_noise3 = sample\n",
        "      rms = math.sqrt(np.mean(y_noise3**2))\n",
        "      noise3 = np.random.normal(0, rms, y_noise3.shape[0])\n",
        "      y_noise3 = y_noise3 + noise3 + noise3\n",
        "      spec_t3 = librosa.feature.melspectrogram(\n",
        "        y= sample,\n",
        "        sr=sr,\n",
        "        n_fft= 1024,\n",
        "        hop_length= 512,\n",
        "        n_mels= 128,\n",
        "        fmax= 8000,\n",
        "        win_length= 400\n",
        "      )\n",
        "      spec_t3 = librosa.power_to_db(spec_t3).transpose()\n",
        "      div = (spec_t3.shape[0] // 75)\n",
        "      size_new = int(div*75)\n",
        "      spec_t3 = spec_t3[:size_new]\n",
        "      spec_t3 = spec_t3.reshape((div, 75, 128))\n",
        "      for k in range(div):\n",
        "        labels.append(lab)\n",
        "      spectrograms = np.concatenate((spectrograms, spec_t3))\n",
        "\n",
        "\n",
        "#Mostramos el espectrograma.\n",
        "plt.figure()\n",
        "spec = spec.reshape((75,128))\n",
        "librosa.display.specshow(spec)\n",
        "plt.colorbar()"
      ],
      "metadata": {
        "id": "URWPBCwzkGE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Verifico que el número de labels coincidan con el número de espectrogramas.\n",
        "spectrograms = spectrograms[1:,:,:] #le resto a la posición 1 de spectrograms 1 valor que le añadí al inicializar la variable con np.ceros\n",
        "print(spectrograms.shape, \"Spectrograms array\")\n",
        "print(len(labels))\n",
        "labels = np.asarray(labels, dtype=int) #Convierte la lista diferentes en una matriz NumPy con el tipo de datos entero (dtype=int).\n",
        "\n",
        "\"\"\"La función TensorDataset() crea un objeto de conjunto de datos PyTorch que se utiliza comúnmente para la entrada de modelos de aprendizaje\n",
        "automático de PyTorch. En general, este objeto se utiliza para que los datos puedan ser entregados al modelo en lotes durante el entrenamiento.\"\"\"\n",
        "dataset = TensorDataset(\n",
        "    torch.from_numpy(spectrograms), #La función torch.from_numpy convierte los datos desde arreglos de NumPy a tensores de PyTorch\n",
        "    torch.from_numpy(labels),\n",
        ")\n",
        "\n",
        "\"\"\"En resumen, el código crea un objeto TensorDataset que contiene los datos de entrada y salida para un modelo de aprendizaje automático\n",
        "y los almacena como tensores de PyTorch.\"\"\""
      ],
      "metadata": {
        "id": "UMTzCs6ApKsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#clase utilizada para cargar datos almacenados en formato pickle en Python (forma de serializar y guardar objetos de Python en archivos binarios)\n",
        "\n",
        "def dataLoaderPickle(filename_train, filename_val, batch):\n",
        "    \"\"\"filename_train, que es el nombre del archivo que contiene los datos de entrenamiento filename_val, que es el nombre del archivo que contiene los datos de validación\n",
        "    y batch que es el tamaño de lote utilizado para cargar los datos en el modelo.\"\"\"\n",
        "\n",
        "    #Cargamos los datos de filename_train (datos para entrenar) en dataset_train\n",
        "    infile = open(filename_train, 'rb')\n",
        "    dataset_train = pickle.load(infile)\n",
        "    infile.close()\n",
        "\n",
        "    #Cargamos los datos de filename_val (datos para validar) en dataset_val\n",
        "    infile = open(filename_val, 'rb')\n",
        "    dataset_val = pickle.load(infile)\n",
        "    infile.close()\n",
        "    print(\"Training size: \", len(dataset_train))\n",
        "    print(\"Validation size: \",len(dataset_val))\n",
        "\n",
        "    train_dataloader = torch.utils.data.DataLoader(\n",
        "        dataset_train,\n",
        "        batch_size = batch,\n",
        "        num_workers=2,\n",
        "        shuffle=True #mezclamos datos de entrenamiento y aumentar la diversidad de las muestras que se utilizan en cada iteración del entrenamiento.\n",
        "    )\n",
        "    val_dataloader = torch.utils.data.DataLoader(\n",
        "        dataset_val,\n",
        "        batch_size=batch,\n",
        "        num_workers=2,\n",
        "        shuffle=True\n",
        "    )\n",
        "    return train_dataloader, val_dataloader"
      ],
      "metadata": {
        "id": "E3fZ02O5qPkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#clase utilizada para guardar datos en un formato específico en Python\n",
        "def dataSaver(dataset, filename):\n",
        "    #filename = 'Scripted_PHQ8_Eng'\n",
        "    outfile = open(filename, 'wb')\n",
        "    pickle.dump(dataset, outfile)\n",
        "    outfile.close()"
      ],
      "metadata": {
        "id": "u7uCz9V1razv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Se guarda el conjunto de datos dataset en un archivo llamado \"Dataset_Aleman\".\n",
        "ruta_archivo = '/content/drive/MyDrive/Datasets/Dataset_Toronto_ruido_m6'\n",
        "dataSaver(dataset, ruta_archivo)"
      ],
      "metadata": {
        "id": "Dkrd9Ug2rfjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Para mostrar el histograma donde se muestra la cantidad de datos por emoción\n",
        "plt.close('all')\n",
        "n, bins, patches = plt.hist(x=labels, bins='auto', color='#0504aa' , alpha=0.7, rwidth=0.85)\n",
        "plt.grid(axis='y', alpha=0.75)\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Is my data balanced?')\n",
        "plt.text(23,45,r'$\\mu15, b=3$')\n",
        "maxfreq = n.max()\n",
        "plt.ylim(ymax=np.ceil(maxfreq/10)*10 if maxfreq % 10 else maxfreq + 10)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qv47TAYBrrPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creación del modelo DL para clasificación de emociones\n",
        "La arquitectura del modelo de este TFG consiste en combinar 4 redes neuronales diferentes: ResCNN, CNN, LSTM (como RNN) y FC."
      ],
      "metadata": {
        "id": "MDLgBE63sS2-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train"
      ],
      "metadata": {
        "id": "MkzRxsiVuu6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Training\n",
        "\n",
        "def trainXD(epochs, model_mlt, train_loader, optimizer, criterion, iter_meter):\n",
        "    acc_emotion_list = []\n",
        "    loss_list = []\n",
        "    label_emotion_list = []\n",
        "    predicted_list_emotion = []\n",
        "    loss_tot_list = []\n",
        "    accuracy_emotion = []\n",
        "\n",
        "    #epochs: El número de épocas para entrenar el modelo, cantidad de veces que el modelo recorrerá todo el conjunto de datos de entrenamiento durante el proceso de entrenamiento.\n",
        "    #model_mlt: El modelo de aprendizaje profundo que se entrena.\n",
        "    #train_loader: El cargador de datos que proporciona los datos de entrenamiento.\n",
        "    #optimizer: El optimizador que se utiliza para salir de train cuando llega a un punto sin salida.\n",
        "    #criterion: La función de pérdida que se utiliza para calcular la pérdida entre las predicciones del modelo y las etiquetas reales.\n",
        "    #iter_meter: Un medidor que se utiliza para realizar un seguimiento del número de iteraciones a través de los datos de entrenamiento.\n",
        "\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    Por cada época pasa la variable de entrada por todo el modelo, este saca una previsión, le pide que calcule la variable pérdida que es el loss\n",
        "    y luego vuelve para atrás y vuelve a hacerlo todo en función de esa variable\n",
        "    \"\"\"\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model_mlt.train() #notebook\n",
        "        total_training_loss = 0\n",
        "\n",
        "        for x,y in train_loader:\n",
        "            inputs = x\n",
        "            label_emotion = y.long()\n",
        "            label_emotion_list.extend(label_emotion.detach().numpy())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            phq8 = model_mlt(inputs.float())\n",
        "            phq8 = phq8.squeeze(0)\n",
        "\n",
        "            predicted_list_emotion.extend((torch.max(torch.exp(F.log_softmax(phq8,dim=1)),1)[1]).detach().numpy())\n",
        "\n",
        "            loss = criterion_emotion(phq8,label_emotion) #esto hay que cambiarlo al estado de ánimo\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            iter_meter.step()\n",
        "            total_training_loss += loss\n",
        "\n",
        "            loss_list.append(loss.item())\n",
        "            #Track accuracy\n",
        "            total_emotion = label_emotion.size(0)\n",
        "            _, predicted = torch.max(phq8.data,1)\n",
        "            correct_emotion = (predicted == label_emotion).sum().item()\n",
        "            acc_emotion_list.append(correct_emotion/total_emotion)\n",
        "\n",
        "        if epoch == (epochs-1):\n",
        "          print('Train Epoch: {} \\tLoss: {:.4f}\\tAccuracy: {:.4f}'.format(\n",
        "              epoch,\n",
        "              np.mean(loss_list),\n",
        "              np.mean(acc_emotion_list)\n",
        "          ))\n",
        "        loss_tot_list.append(np.mean(loss_list))\n",
        "        accuracy_emotion.append(np.mean(acc_emotion_list))\n",
        "\n",
        "\n",
        "    #Printing Confusion Matrix\n",
        "    \"\"\" Matriz que en el eje x los valores que quiero dar y en el y lo que deberían dar y van sumando \"\"\"\n",
        "\n",
        "    classes_emotion = ('0','1','2','3','4','5','6')\n",
        "    cf_matrix_emotion = confusion_matrix(label_emotion_list, predicted_list_emotion)\n",
        "    df_cm = pd.DataFrame(cf_matrix_emotion/np.sum(cf_matrix_emotion)*10,index = [i for i in classes_emotion], columns=[i for i in classes_emotion])\n",
        "    plt.figure(figsize=(12,7))\n",
        "    sn.heatmap(df_cm, annot=True)\n",
        "    plt.savefig('Confusion Matrix emotionression Training')\n",
        "\n",
        "    fig, axs = plt.subplots(2)\n",
        "    axs[0].plot(range(epochs), loss_tot_list)\n",
        "    axs[0].set_title('Training Loss')\n",
        "    axs[0].set(xlabel= 'Epoch', ylabel='Loss')\n",
        "    axs[1].plot(range(epochs), accuracy_emotion)\n",
        "    axs[1].set_title('Training Accuracy')\n",
        "    axs[1].set(xlabel= 'Epoch', ylabel='Accuracy')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "38sRsoGkujDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Test"
      ],
      "metadata": {
        "id": "F2dp7K2Iu2Fn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def testXD(epochs, model_mlt, train_loader, optimizer, criterion, iter_meter):\n",
        "    acc_emotion_list = []\n",
        "    loss_list = []\n",
        "    label_emotion_list = []\n",
        "    predicted_list_emotion = []\n",
        "    loss_tot_list = []\n",
        "    accuracy_emotion = []\n",
        "    epochs = 1\n",
        "    #Training\n",
        "    for epoch in range(epochs):\n",
        "        model_mlt.train()\n",
        "        total_training_loss = 0\n",
        "\n",
        "        for x,y in train_loader:\n",
        "            inputs = x\n",
        "            label_emotion = y.long()\n",
        "            label_emotion_list.extend(label_emotion.detach().numpy())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            phq8 = model_mlt(inputs.float())\n",
        "            phq8 = phq8.squeeze(0)\n",
        "\n",
        "            predicted_list_emotion.extend((torch.max(torch.exp(F.log_softmax(phq8,dim=1)),1)[1]).detach().numpy())\n",
        "\n",
        "            loss = criterion_emotion(phq8,label_emotion)\n",
        "\n",
        "            '''\n",
        "            A diferencia del Training, esto está comentado porque lo que no hace es ir para atrás, porque no hace falta\n",
        "            aquí le doy el resultado por toda la red y que me diga lo que vale, no quiero que me recompute los valores, sino\n",
        "            que quiero ver que lo que he hecho está bien\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            iter_meter.step()'''\n",
        "            total_training_loss += loss\n",
        "\n",
        "            loss_list.append(loss.item())\n",
        "            #Track accuracy\n",
        "            total_emotion = label_emotion.size(0)\n",
        "            _, predicted = torch.max(phq8.data,1)\n",
        "            correct_emotion = (predicted == label_emotion).sum().item()\n",
        "            acc_emotion_list.append(correct_emotion/total_emotion)\n",
        "\n",
        "        print('Test Epoch: {} \\tLoss: {:.4f}\\temotionression Accuracy: {:.4f}'.format(\n",
        "            epoch,\n",
        "            np.mean(loss_list),\n",
        "            np.mean(acc_emotion_list)\n",
        "        ))\n",
        "        loss_tot_list.append(np.mean(loss_list))\n",
        "        accuracy_emotion.append(np.mean(acc_emotion_list))\n",
        "\n",
        "\n",
        "    #Printing Confusion Matrix\n",
        "    classes_emotion = ('0','1','2','3','4','5','6')\n",
        "    cf_matrix_emotion = confusion_matrix(label_emotion_list, predicted_list_emotion)\n",
        "    df_cm = pd.DataFrame(cf_matrix_emotion/np.sum(cf_matrix_emotion)*10,index = [i for i in classes_emotion], columns=[i for i in classes_emotion])\n",
        "    plt.figure(figsize=(12,7))\n",
        "    sn.heatmap(df_cm, annot=True)\n",
        "    plt.savefig('Confusion Matrix Testing')\n",
        "\n",
        "    fig, axs = plt.subplots(2)\n",
        "    axs[0].plot(range(epochs), loss_tot_list)\n",
        "    axs[0].set_title('Test Loss')\n",
        "    axs[0].set(xlabel= 'Epoch', ylabel='Loss')\n",
        "    axs[1].plot(range(epochs), accuracy_emotion)\n",
        "    axs[1].set_title('Test Accuracy')\n",
        "    axs[1].set(xlabel= 'Epoch', ylabel='Accuracy')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "gnD3PrhFu1HB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Redes neuronales usadas"
      ],
      "metadata": {
        "id": "YAwkczOQx5w9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ResCNN"
      ],
      "metadata": {
        "id": "KjBl4wBJyGA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#clase de red neuronal convolucional residual que consta de dos capas convolucionales, una capa de normalización de lotes, una función\n",
        "#de activación ReLU, una capa de regularización Dropout\n",
        "\n",
        "class ResCNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Primero declaro la clase ResCNN, una red CNN es una red convolucional\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, stride=1, downsample= None):\n",
        "\n",
        "        \"\"\"\n",
        "        Le entra el número de canales que tengo 128 (banda frecuencial)\n",
        "        y qué es lo que quiero que salga (le puedo dar el valor que quiera pero en este caso será 32)\n",
        "        \"\"\"\n",
        "        super(ResCNN, self).__init__()\n",
        "\n",
        "        #BatchNorm1d --> La normalización de lotes se aplica típicamente después de la salida de una capa convolucional\n",
        "        #Conv1d --> Convolución de 1D\n",
        "        #RELU --> Función de activación\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.BatchNorm1d(in_channels),\n",
        "            nn.Conv1d(in_channels, out_channels, kernel_size=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv1d(out_channels, out_channels, kernel_size=1)\n",
        "        )\n",
        "\n",
        "        #downsample --> reducción de la tasa de muestreo, es decir, disminución de la resolución de una señal o conjunto de datos\n",
        "        #(implica tomar muestras en intervalos menos frecuentes o eliminar ciertas muestras para reducir la cantidad de información)\n",
        "        self.downsample = downsample\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.out_channels = out_channels #almacena el número de canales de salida de la capa convolucional.\n",
        "\n",
        "        self.dropout = nn.Dropout(p=0.2)\n",
        "        \"\"\" técnica de regularización para prevenir el sobreajuste. Durante el entrenamiento, se desactivan aleatoriamente el 20%  de las unidades de la capa de entrada impidiendo\n",
        "        así que las unidades de la red dependan demasiado de las demás y, en última instancia, promueve un aprendizaje más robusto y generalizable.\n",
        "        \"\"\"\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        output = self.conv1(x)\n",
        "        output = self.conv2(output)\n",
        "        if self.downsample:\n",
        "            residual = self.downsample(x)\n",
        "        output += residual\n",
        "        output = self.relu(output)\n",
        "        #output = self.dropout(output)\n",
        "        return output\n",
        "\n",
        "    \"\"\"El código implementa una operación de residual connection en una red neuronal convolucional, donde se suma la entrada original con la salida de las capas convolucionales.\n",
        "    Luego, se aplica una función de activación ReLU al resultado y se devuelve como salida.\"\"\""
      ],
      "metadata": {
        "id": "tVYz0ogTx-CM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###LSTM"
      ],
      "metadata": {
        "id": "43dLuYGKyMYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"El código define una capa LSTM (Long Short-Term Memory) en una red neuronal recurrente (RNN) con ciertas dimensiones y capas.\n",
        "En el método forward, se pasa la entrada a través de la capa LSTM y se devuelve el resultado.\"\"\"\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, n_rnn_dim, h_rnn_layer, n_rnn_layers):\n",
        "        super(LSTM,self).__init__()\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size= n_rnn_dim,\n",
        "            hidden_size= h_rnn_layer,\n",
        "            num_layers= n_rnn_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.lstm(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "qKyCLoB6yPm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###FC"
      ],
      "metadata": {
        "id": "tKmnLb88yT0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Esta función define la red FC, la cual realiza una transformación lineal de la entrada, aplica una función de activación para introducir no linealidad\n",
        "y, opcionalmente, normaliza los valores por lotes para mejorar el rendimiento de la red neuronal.\"\"\"\n",
        "\n",
        "class FullyConnected(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(FullyConnected, self).__init__()\n",
        "        self.fc = nn.Linear(in_features=in_channels, out_features=out_channels)\n",
        "        self.activation = nn.ReLU()\n",
        "        self.bn = nn.BatchNorm1d(in_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #x = self.bn(x)\n",
        "        x = self.fc(x)\n",
        "        x = self.activation(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "mRbV7fEQyVXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Arquitectura del modelo DeepSpeech"
      ],
      "metadata": {
        "id": "Ro5RAxD9yaAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepSpeech(nn.Module):#my deepspe.. puede usar las funciones de nn.Module\n",
        "    \"\"\"Donde creo mi modelo, junto todas estas redes neuronales definidas anteriormente\"\"\"\n",
        "\n",
        "\n",
        "    def __init__(self, n_cnn_layers, n_rnn_dim, h_rnn_layer, n_rnn_layers, n_classes, stride=2, dropout=0.1):\n",
        "        super(DeepSpeech, self).__init__() #aquí le indico a DeppSpeech que nn.Module es su madre y puedo usar sus funciones\n",
        "\n",
        "        self.dense = nn.Conv1d(75, 32, 1) #mi danse va a tomar el valor que devuelva la función nn.Conv1d\n",
        "        \"\"\"De la primera red que tengo (convolución) quiero que me pase de los 75 que tengo de entrada a 32 donde el size del filtro\n",
        "        kernel=1 \"\"\"\n",
        "        \"\"\"Estos tensores de salida representarán las características aprendidas por los filtros en cada canal de salida y se utilizarán como\n",
        "        entrada para las capas subsiguientes de la red neuronal para realizar tareas como clasificación\"\"\"\n",
        "\n",
        "        self.cnn = nn.Sequential(*[\n",
        "            ResCNN(\n",
        "                32,\n",
        "                32\n",
        "            )\n",
        "            for i in range(n_cnn_layers)\n",
        "        ])\n",
        "\n",
        "        #nn.linear(a,b,TRUE), a-->tamaño muestra de entrada,  b--> tamaño muestra de salida\n",
        "        #se encuentra en la fase de procesamiento final de la red neuronal, donde se combinan las características extraídas para generar la salida deseada.\n",
        "        ## n_rnn_dim quiero que la dimensión de LSTM sea de 256 muestras\n",
        "\n",
        "        self.dense2 = nn.Linear(\n",
        "            32*128,\n",
        "            n_rnn_dim #256\n",
        "        )\n",
        "\n",
        "        \"\"\" Después de esto quiero que me haga un filtro lineal y me pase de estos 32*128 (estos 1128 porque antes tenía una matriz de\n",
        "        128*75 y ahora la tengo de 128*32 --> álgebra lineal) que tengo.\"\"\"\n",
        "\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size= n_rnn_dim,\n",
        "            hidden_size= h_rnn_layer,\n",
        "            num_layers= n_rnn_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "        )\n",
        "\n",
        "        \"\"\"Del LSTM sale lo mismo, los 128*32 y esto sigue siendo más que el número de clases por lo que paso al FullyConnected, bajando así el número de clases\"\"\"\n",
        "        #Bajo la matriz al número de clases\n",
        "        self.fc = FullyConnected(\n",
        "            in_channels= n_rnn_dim,\n",
        "            out_channels= n_classes\n",
        "        )\n",
        "\n",
        "    #Se definen muchos transpose para trasponer ya que cada ves que se hace cambio de datos hay que hacerlo\n",
        "    def forward(self, x):\n",
        "        batch,freq,width = x.shape\n",
        "        x = self.dense(x)\n",
        "        x = self.cnn(x)\n",
        "        x = x.view(x.size(0),x.size(1)*x.size(2))\n",
        "        #x = x.transpose(0,1)\n",
        "        x = self.dense2(x)\n",
        "        #x = x.transpose(0,1)\n",
        "        x,_ = self.lstm(x)\n",
        "        #x = self.atten(x)\n",
        "        x = x.transpose(0,1)\n",
        "        #x = x[:,-1]\n",
        "        x = x.transpose(0,1)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "okd3gzDlyfVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Optimizadores"
      ],
      "metadata": {
        "id": "GdcmKM9Ny3ot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Aquí se define un optimizador que se llama AdamW que es un tipo de algoritmo que existe en internet, que cuando entro a un callejón de salida cómo sale?, sirve para evitar estos puntos\"\"\"\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "def optimizerNet(model, hparams, train_loader):\n",
        "    optimizer = optim.AdamW(\n",
        "        model.parameters(),\n",
        "        hparams['learning_rate']\n",
        "    )\n",
        "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer,\n",
        "        max_lr=hparams['learning_rate'],\n",
        "        steps_per_epoch= int(len(train_loader)),\n",
        "        epochs= hparams['epochs'],\n",
        "        anneal_strategy= 'linear'\n",
        "    )\n",
        "    return optimizer, scheduler\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "HjhUX2dsy5q1",
        "outputId": "990eb916-4d5a-4726-d99a-0637e2447fe7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function WeakSet.__init__.<locals>._remove at 0x7f7dc941bf40>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/_weakrefset.py\", line 39, in _remove\n",
            "    def _remove(item, selfref=ref(self)):\n",
            "KeyboardInterrupt: \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-66e6ebab284a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \"\"\"Esto es llamada a un optimizador que se llama AdamW que es un tipo de algoritmo que existe en internet, que cuando\n\u001b[1;32m      3\u001b[0m entro a un callejón de salida cómo sale?, sirve para evitar estos puntos\"\"\"\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0moptimizerNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m \u001b[0;31m# Shared memory manager needs to know the exact location of manager executable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1146\u001b[0;31m \u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initExtension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmanager_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1147\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mmanager_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0m_lazy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_check_capability\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m \u001b[0m_lazy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_check_cubins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_call\u001b[0;34m(callable, **kwargs)\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0;31m# Don't store the actual traceback to avoid memory cycle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m             \u001b[0m_queued_calls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0m_lazy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_check_capability\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/traceback.py\u001b[0m in \u001b[0;36mformat_stack\u001b[0;34m(f, limit)\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mformat_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/traceback.py\u001b[0m in \u001b[0;36mformat_list\u001b[0;34m(extracted_list)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mwhose\u001b[0m \u001b[0msource\u001b[0m \u001b[0mtext\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \"\"\"\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mStackSummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextracted_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/traceback.py\u001b[0m in \u001b[0;36mformat\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    438\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m             row.append('  File \"{}\", line {}, in {}\\n'.format(\n\u001b[0m\u001b[1;32m    441\u001b[0m                 frame.filename, frame.lineno, frame.name))\n\u001b[1;32m    442\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"La clase \"IterMeter\" es utilizada para realizar un seguimiento del progreso de las iteraciones. Inicialmente, se establece el valor en 0 y luego\n",
        "se incrementa en 1 con cada llamada a la función \"step\". La función \"get\" devuelve el valor actual almacenado en \"val\".\"\"\"\n",
        "\n",
        "\n",
        "class IterMeter(object):\n",
        "    \"\"\"Sirve para que vaya avanzando\"\"\"\n",
        "    #Keeping track of iterations\n",
        "    def __init__(self):\n",
        "       self.val = 0 #inicializo\n",
        "\n",
        "    def step(self):\n",
        "        self.val += 1 #le va sumando 1 a 1 a val\n",
        "\n",
        "    def get(self):\n",
        "        return self.val #devuelve la posición de val actual"
      ],
      "metadata": {
        "id": "C3hHuVWFy6b1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##KFold + Hiperparámetros"
      ],
      "metadata": {
        "id": "i9fHp3pGzJaU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Aquí defino mis hyperparámetros, estos son una serie de números que tengo que ir optimizando para ver cuál es mejor para mi modelo\"\"\"\n",
        "\n",
        "\n",
        "#Constantes\n",
        "\n",
        "learning_Rate = 0.0005\n",
        "\"\"\"lo mucho que le dejo aprender, lo mucho que le dejo que se acerque a este callejón sin salida (entre 0-1), cuánto más grande\n",
        "sea el rate más fácil es que no llegue al callejón de salida pero menos aprende \"\"\"\n",
        "\n",
        "batch_size = 128\n",
        "\"\"\"El tamaño de cuántas muestras entran, cuántas más muestras entren más aprende pero más lento va\"\"\"\n",
        "\n",
        "epochs = 300 #El número de veces que entreno\n",
        "\n",
        "\"\"\"KFold es que cojo el dataset y hago reparticiones aleatrorias, 5 particiones (porque la K=5) distintas, para asegurar que justamente no haya\n",
        "entrenado con unos datos claves que no funcionan o con una convinación clave que funciona\n",
        "\"\"\"\n",
        "k=5 #entreno 90% y validación 10\n",
        "splits=KFold(n_splits=k,shuffle=True,random_state=42)\n",
        "#shuffle=True mezcla todo\n",
        "#random_state = que tenga un ángulo 42 de libertad\n",
        "\n",
        "\n",
        "foldperf={}\n",
        "\n",
        "\"\"\"Es lo que le tengo que mandar al modelo para que lo números de antes, intchanell, outchanell aquí es donde lo declaro\"\"\"\n",
        "hparams = {\n",
        "    \"n_cnn_layers\": 6, #quiero que me haga 6 layers de CNN\n",
        "    \"n_rnn_layers\": 1, #quiero que me haga sólo 1 vez la de LSTM\n",
        "    \"rnn_dim\": 256, #quiero que la dimensión de LSTM sea de 256 muestras\n",
        "    \"h_rnn_layers\": 128, #me lo mires 128 veces--> vector clave respecto a los 256 que tenía antes\n",
        "    \"n_class\": 7, #quiero que hayan 7 clases (en este caso, en otros datasets las que hayan)\n",
        "    \"n_feats\": 64,\n",
        "    \"stride\": 2,\n",
        "    \"dropout\": 0.3,\n",
        "    \"learning_rate\": learning_Rate, #0.0005\n",
        "    \"batch_size\": batch_size, #128 muestras\n",
        "    \"epochs\": epochs #300\n",
        "}\n",
        "\n",
        "\n",
        "#Optimizing Model\n",
        "weights = [1.0,1.0,1.0,1.0,1.0,1.0,1.0]\n",
        "\"\"\"Con los weights penalizo las clases más pobladas\n",
        "Lo que hace es que donde tenga menos datos puedo hacer que cuando calcule la probabilidad de esta sea más baja, penalizar donde tenga más datos\n",
        "\"\"\"\n",
        "class_weights = torch.FloatTensor(weights)\n",
        "\n",
        "criterion_emotion = nn.CrossEntropyLoss(weight=class_weights) #.to(device)\n",
        "\"\"\"Esta es la función con la que voy a evaluar que funciona, hay muchos tipos de funciones de pérdidas (MCE...), en este caso hacemos la entropía\n",
        "cruzada\n",
        "\"\"\"\n",
        "criterion_anx = nn.CrossEntropyLoss()\n",
        "iter_meter = IterMeter()"
      ],
      "metadata": {
        "id": "71D_OUTQzLrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Entrenamiento del modelo"
      ],
      "metadata": {
        "id": "u01d539TzpSk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Aquí entreno 5 veces el modelo, con 5 tipos de datos distintos\n",
        "#En cada iteración, el tamaño de train_idx será 4/5 del tamaño total de los datos y el tamaño de val_idx será 1/5\n",
        "\n",
        "ruta_archivo = '/content/drive/MyDrive/Datasets/Dataset_Toronto_ruido_m6'\n",
        "infile = open(ruta_archivo, 'rb')\n",
        "Dataset_Toronto_ruido_m6 = pickle.load(infile)\n",
        "infile.close()\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(splits.split(np.arange(len(Dataset_Toronto_ruido_m6)))): #90% entreno y 10% testing\n",
        "    print('Fold {}'.format(fold+1)) #En total habrán 5 Folds que serán los difernetes conjuntos de datasets que entrene.\n",
        "\n",
        "   #pasamos las variables a tipo sampler\n",
        "    train_sampler = SubsetRandomSampler(train_idx) #clase importada de la librería torch\n",
        "    test_sampler = SubsetRandomSampler(val_idx)\n",
        "    \"\"\"Con el SubsetRandomSampler, cogeríamos unos x valores randoms del total del dataset train_idx, como después de train_idx no tiene ningún\n",
        "    valor, pone por predeterminado el NONE, es decir, que pilla todos los datos de train_idx por lo que en realidad estamos pasando de un tipo\n",
        "    de variable a varialbe tipo sampler\"\"\"\n",
        "\n",
        "    #Cargamos el dataset de 128 en 128 de forma random (definida en train_sampler con la función SubsetRandomSampler)\n",
        "    train_loaderUns = DataLoader(Dataset_Toronto_ruido_m6, batch_size=batch_size, sampler=train_sampler)\n",
        "    test_loaderUns = DataLoader(Dataset_Toronto_ruido_m6, batch_size=batch_size, sampler=test_sampler)\n",
        "\n",
        "    #Llama al init de la clase DeepSpeech\n",
        "    model = DeepSpeech(\n",
        "        hparams['n_cnn_layers'],\n",
        "        hparams['rnn_dim'],\n",
        "        hparams['h_rnn_layers'],\n",
        "        hparams['n_rnn_layers'],\n",
        "        hparams['n_class'],\n",
        "        hparams['stride'],\n",
        "        hparams['dropout']\n",
        "    )\n",
        "\n",
        "    optimizer, scheduler = optimizerNet(model, hparams, train_loaderUns)\n",
        "\n",
        "    trainXD(epochs, model, train_loaderUns, optimizer, criterion_emotion, iter_meter)\n",
        "    testXD(epochs, model, test_loaderUns, optimizer, criterion_anx, iter_meter)\n",
        "\n"
      ],
      "metadata": {
        "id": "IvsJciJozrgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Algoritmo de almacenamiento del modelo"
      ],
      "metadata": {
        "id": "ihkOJPQ1B0tu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ruta_modelo = '/content/drive/MyDrive/Datasets/modelo_entrenado_TORONTO.pth'\n",
        "torch.save(model.state_dict(), ruta_modelo)\n"
      ],
      "metadata": {
        "id": "wYWOfpz6CzMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ruta_modelo = '/content/drive/MyDrive/Datasets/modelo_entrenado_TORONTO.pth'\n",
        "modelo_cargado = DeepSpeech(...)\n",
        "modelo_cargado.load_state_dict(torch.load(ruta_modelo))\n",
        "modelo_cargado.eval()"
      ],
      "metadata": {
        "id": "iVt7pa71Cz8-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "_nzbQKF2fon-",
        "tqJILDFIubs5",
        "C-HROeSoP-0A"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}